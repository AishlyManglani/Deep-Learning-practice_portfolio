# -*- coding: utf-8 -*-
"""2_finetune_pretrained_transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YAtRHmoKlVNWjMYQNqol4mECsptWMGAx

# ðŸ”§ Fine-tuning a Pretrained BERT Model on IMDb Reviews
"""

!pip install transformers datasets

!pip install transformers[tf] # Install transformers with TensorFlow support.

from datasets import load_dataset
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification

# Load and split dataset (90% train, 10% validation)
raw = load_dataset("imdb")
splits = raw["train"].train_test_split(test_size=0.1, seed=42)
train_raw = splits["train"]
val_raw = splits["test"]
test_raw = raw["test"]

# Prepare tokenizer & model
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFAutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2
)

# Tokenization helper (batched)
def tokenize_batch(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=128
    )

# %%
# Preprocess and subsample for faster training
train_small = (
    train_raw
    .map(tokenize_batch, batched=True)
    .select(range(5000))
    .remove_columns(["text"])
    .rename_column("label", "labels")
)
val_dataset = (
    val_raw
    .map(tokenize_batch, batched=True)
    .remove_columns(["text"])
    .rename_column("label", "labels")
)

# Convert to tf.data datasets
train_ds = train_small.to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["labels"],
    shuffle=True,
    batch_size=32
)
val_ds = val_dataset.to_tf_dataset(
    columns=["input_ids", "attention_mask"],
    label_cols=["labels"],
    batch_size=32
)

test_ds = (
    test_raw
    .map(tokenize_batch, batched=True)
    .remove_columns(["text"])
    .rename_column("label", "labels")
    .to_tf_dataset(
        columns=["input_ids", "attention_mask"],
        label_cols=["labels"],
        batch_size=32
    )
)

# Compile model
optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name="accuracy")]
model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

# Callbacks for training
callbacks = [
    tf.keras.callbacks.ModelCheckpoint(
        "best_model",               # Saved in TensorFlow SavedModel format (directory)
        save_best_only=True,
        monitor="val_accuracy",
        save_weights_only=True       # Only save weights (avoid HDF5 format error)
    ),
    tf.keras.callbacks.EarlyStopping(
        monitor="val_accuracy",
        patience=1,
        restore_best_weights=True
    )
]

# %%
# Train (reduced to 2 epochs)
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=2,
    callbacks=callbacks
)

# Evaluate on test set
loss, accuracy = model.evaluate(test_ds)
print(f"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}")