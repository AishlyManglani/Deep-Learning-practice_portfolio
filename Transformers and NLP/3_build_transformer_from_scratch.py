# -*- coding: utf-8 -*-
"""3_build_transformer_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VDKTvdTiXZUUL4hcm0bUeYeL6bQgMWaG

# üèóÔ∏è Build and Train a Transformer from Scratch on AG News
"""

!pip install tensorflow datasets

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from datasets import load_dataset

# Load AG News dataset
dataset = load_dataset('ag_news')

# Prepare text vectorization
max_tokens = 20000
max_len = 200

vectorize_layer = layers.TextVectorization(
    max_tokens=max_tokens,
    output_mode='int',
    output_sequence_length=max_len
)
train_text = dataset['train']['text']
vectorize_layer.adapt(train_text)

# Transformer block definition
class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super().__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = keras.Sequential([
            layers.Dense(ff_dim, activation='relu'),
            layers.Dense(embed_dim),
        ])
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

# Build the model
embed_dim = 32
num_heads = 2
ff_dim = 32

def build_model():
    inputs = layers.Input(shape=(None,), dtype='int64')
    x = layers.Embedding(input_dim=max_tokens, output_dim=embed_dim)(inputs)
    # Pass training=False to the TransformerBlock call
    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x, training=False)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(0.1)(x)
    outputs = layers.Dense(4, activation='softmax')(x)
    return keras.Model(inputs, outputs)

model = build_model()
model.compile('adam', 'sparse_categorical_crossentropy', metrics=['accuracy'])

# Prepare data pipelines
def to_tf(dataset, split):
    ds = dataset[split].map(lambda x: {'text': vectorize_layer(x['text'])}, batched=False) # Wrap the output in a dictionary
    labels = dataset[split]['label']
    # Extract the 'text' values before creating the tf.data.Dataset
    text_values = [x['text'] for x in ds]
    return tf.data.Dataset.from_tensor_slices((text_values, labels)).batch(32)

train_ds = to_tf(dataset, 'train')
val_ds = to_tf(dataset, 'test')