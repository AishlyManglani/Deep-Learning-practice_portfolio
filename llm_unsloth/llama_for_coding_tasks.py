# -*- coding: utf-8 -*-
"""llama for coding tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17qleKQzG6twtQaJDnFg2hrGpTCNzGsnY
"""

# Install Unsloth
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
!pip install --no-deps "xformers<0.0.27" "trl<0.9.0" peft accelerate bitsandbytes

# Import necessary libraries
from unsloth import FastLanguageModel
import torch
from datasets import load_dataset
from trl import SFTTrainer
from transformers import TrainingArguments

# Load model and tokenizer
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="meta-llama/Meta-Llama-3.1-8B",
    max_seq_length=2048,
    dtype=None,
    load_in_4bit=True,
)

# Load a coding dataset (using HumanEval for code examples)
dataset = load_dataset("openai_humaneval", split="test")

# Format dataset for fine-tuning
# Format dataset for fine-tuning
def format_coding_example(example):
    # Extract the function signature and docstring from the prompt
    prompt = example["prompt"]

    # Format as a chat conversation
    messages = [
        {"role": "system", "content": "You are a helpful coding assistant that writes clean, efficient code."},
        {"role": "user", "content": f"Please complete this function:\n\n{prompt}"}
    ]

    # If there's a canonical solution, add it as the assistant's response
    if "canonical_solution" in example:
        messages.append({"role": "assistant", "content": example["canonical_solution"]})

    # Apply the chat template
    # Define a chat template
    chat_template = """{% if messages[0]['role'] == 'system' %}{{messages[0]['content']}}{% endif %}
    {% for message in messages[1:] %}
    {{message['role']}}: {{message['content']}}
    {% endfor %}"""
    example["text"] = tokenizer.apply_chat_template(messages, chat_template=chat_template, tokenize=False) # Pass the chat_template
    return example

formatted_dataset = dataset.map(format_coding_example)


# Prepare model for training
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing=True,
    random_state=42,
)

# Set up training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    weight_decay=0.01,
    logging_steps=10,
    save_steps=50,
    save_total_limit=3,
)

# Create trainer
trainer = SFTTrainer(
    model=model,
    train_dataset=formatted_dataset,
    args=training_args,
    tokenizer=tokenizer,
    packing=False,  # Set to False to avoid packing issues
    dataset_text_field="text",
)

# Train the model
trainer.train()

# Save the model
trainer.save_model("llama-3.1-8b-coding")

# Test the model
FastLanguageModel.for_inference(model)
messages = [
    {"role": "system", "content": "You are a helpful coding assistant that writes clean, efficient code."},
    {"role": "user", "content": "Write a Python function to find the longest common substring between two strings."}
]
prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(prompt, return_tensors='pt').to("cuda")
outputs = model.generate(**inputs, max_new_tokens=512, temperature=0.7)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))